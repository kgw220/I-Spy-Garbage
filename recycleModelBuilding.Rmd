---
title: 'I Spy Garbage: Model Building'
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

I used the data set linked here: <https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification>

I have gone ahead and manually divided the sets of images for each material into training and testing sets, where testing sets compose of the last 20% of images in each folder. None of the images are in any particular order, so this should be fine opposed to doing a randomized split.

Below is an screenshot of how I set up my training data, with each class of images being in their own sub-folder. For this data set, most of the images have the same dimensions, but this is not needed - We will set the images to a specific size later.

![](Screenshot_7.jpg)

It can be quite difficult to gather data manually, which is why I use a data set that is already organized. But in essence, you can repeat the same methodology with your own set of images depending on use case.

Also, ensure you set the working directory to where the training and testing sets of images are saved.

```{r}
rm(list = ls()) # clear all environmental variables that may be leftover

library(formatR)
# These will just allow better formatting in the knitted document
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 75), tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Setup for our model

Let's go ahead and import the libraries we need:

```{r message=FALSE}
# general library in R for data manipulation
library(tidyverse) 

# packages for our image processing
library(keras)
library(tensorflow)
library(reticulate) # defines a python virtual environment
```

We will also go ahead and set the random seed. I also go ahead and set an environmental variable to 0 since it caused a warning when I ran this for the first time.

```{r warning=FALSE}
set_random_seed(220, disable_gpu = TRUE) # setting seed for reproducible results
TF_ENABLE_ONEDNN_OPTS=0 # Turning off any potential floating-point round-off errors 
```

Next, I create a list of each label for all our potential material classifications:

```{r}
label_list <- dir("train/") # get a unique list of all material types 
output_n <- length(label_list)
save(label_list, file="label_list.R")
```

I also go ahead and set some parameters for image rescaling. This will be quite useful since it will mean the images we put into the model after it is built will not require specific formatting from the user.

```{r}
width <- 224
height<- 224
target_size <- c(width, height)
rgb <- 3 #color channels
```

I also will set the path to our training data.

```{r}
path_train <- "train/"
```

The next step is to use some data augmentation. This essentially applies random amounts of blurs or rotations to add some variation to our data, which helps to avoid over-fitting. I will also go ahead and set 20% of our data set as validation for when we build the model.

```{r}
train_data_gen <- image_data_generator(rescale = 1/255, validation_split = .2)
```

Now we use a function called `flow_images_from_directory` to batch process our images with the generator function we defined above. We link this up to our training and validation data, and set some basic parameters.

```{r}
train_images <- flow_images_from_directory(path_train,
                                           train_data_gen,
                                           subset = 'training',
                                           target_size = target_size,
                                           class_mode = "categorical",
                                           shuffle=F,
                                           classes = label_list,
                                           seed = 220)

validation_images <- flow_images_from_directory(path_train,
                                                train_data_gen, 
                                                subset = 'validation',
                                                target_size = target_size,
                                                class_mode = "categorical",
                                                classes = label_list,
                                                seed = 220)
```

I will note that I could have just used separate folders for training and validation, but I just chose to let `keras` do it with a random split.

To check our work so far, let's output a table of our classes in our training images.

```{r}
table(train_images$classes)
```

This corresponds to the number of pictures in each folder, so everything looks great so far!

We can even display an example image:

```{r}
plot(as.raster(train_images[[]][[1]][17,,,]))
```

Note that the first element in the `train_images` object has the pixel values of each image which is a 4D-tensor (number of image, width, height, rgb channel), so we are plotting image number 17 above.

## Training the model:

Given the nature of this project, we are going to be using a convoluted neural network (CNN). I found the following resources helpful to understand how it works, but I will also give my own brief explanation below regarding the basic functionality:

-   Video: "But what is a convolution?" by 3Blue1Brown (<https://youtu.be/KuXjwB4LzSA?si=voPVrXqOyJnRBUGI>)

-   Video: "Convolutional Neural Networks (CNNs) explained" by deeplizard (<https://www.youtube.com/watch?v=YRhxdVk_sIs>)

-   Article: "All about convolutions, kernals, features in CNN" by Abhishek Jain ([https://medium.com/\@abhishekjainindore24/all-about-convolutions-kernels-features-in-cnn-c656616390a1](https://medium.com/@abhishekjainindore24/all-about-convolutions-kernels-features-in-cnn-c656616390a1){.uri})

My explanation: So with images, they are made out of pixels, and are effectively just a grid of pixels (with numbers representing each pixel). For a given convolution layer on a CNN, we take what is considered a kernal, most often a square matrix of values inside, and "slide" (or convolute) it over each group of pixels to get a resulting dot product. This transforms the overall image into something else, which can help identify any particular features, such as maybe colors or edges.

![](25366Convolutional_Neural_Network_to_identify_the_image_of_a_bird.png)

In terms of the size of a kernal, it is usually 1x1, 3x3, or 5x5. I believe this is just simply to avoid a common ML problem of having too much noise in your model if the kernal becomes too big, capturing too much information. In terms of the values/weights in a kernal, this depends on the scenerio and will depend on what features you are trying to capture. This is why kernals are often also termed as 'filters.'

And of course, that is only for one single layer. CNN will have multiple convolution layers with multiple different kernals, and the CNN will be trained to adjust the kernals used for each layer to be able to figure out all the right features of an image to classify it.

With that said, it is important to address a big obstacle here. Neural networks (more specifically, convoluted neural networks) have so many ways they can be trained with lots of different parameters. This offers lots of flexibility, but we don't really know on the best way to set up our model. Essentially, we are stuck with the bias variance tradeoff, where we could build a model that is well known s.t. model variance is low, but there may be violations to the assumptions of that model, causing it to be biased. Conversely, we could have a model that can be quite complicated (thus reducing bias), but since there are so many parameters, it would increase model variance.

For the sake of this project, there is a way we can ensure good baseline results, by using models that were pre-trained in the past for other applications. We choose the xception-network with the weights pre-trained on the ImageNet data set, a large data set used for object recognition. However, we will edit the final layer, since this is the output layer that classifies the images, and of course, we need to adjust it to train for our images of potential trash. This is done by setting the parameter `include_top` to FALSE.

```{r warning=FALSE}
mod_base <- application_xception(weights = 'imagenet', include_top = FALSE, input_shape = c(width, height, 3))
freeze_weights(mod_base) # we freeze the weights so they are no longer trainable, 
```

Below, we define a function that will build that final layer that we left out from the `imagenet` model. I set some parameters to variables that we will use to fine-tune our model later.

```{r}
model_function <- function(learningRate = 0.0001, dropoutrate=0.2, 
                            n_dense=1024){
  
  k_clear_session() #resets all generated states

  model <- keras_model_sequential() %>%
    mod_base %>% 
    layer_global_average_pooling_2d() %>% 
    layer_dense(units = n_dense) %>%
    layer_activation("relu") %>%
    layer_dropout(rate = dropoutrate, seed = 220) %>% 
    layer_dense(units=output_n, activation="softmax")
  
  model %>% compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(learning_rate = learningRate),
    metrics = "accuracy"
  )
  
  return(model)
  
}
```

Now, we let the model compile with those base values and inspect the architecture.

```{r}
model <- model_function()
model
```

We can see that we have 20 million parameters from the xception model that are non-trainable, since we only train the top layer, which is a 1024 x 1 feature vector that is densely connected to the output classification layer. This is essentially 1024 variables, which take on certain values depending on if our image has certain properties, and all of this is trained during the convolution layers of the xception network (aka the 'black box').

With that said, let is train the model with our training data now!

```{r warning=FALSE}
batch_size <- 32 # number of training samples fed to our neural network at once
epochs <- 6 # number of times our entire training data set is passed through the neural network

hist <- model %>% fit_generator(
  train_images,
  steps_per_epoch = train_images$n %/% batch_size, 
  epochs = epochs, 
  validation_data = validation_images,
  validation_steps = validation_images$n %/% batch_size,
  verbose = 2
)
```

![](modelResults.jpg)

Above is a corresponding plot that RStudio shows of the accuracy over the 6 epochs (I just have a screenshot since this does not appear in a notebook file). We can see that our accuracy is around 80%, which isn't bad, but there is certainly room for improvement.

## Testing our model

First, let's now use the images we set aside for testing in our model, to see how it performs, with the same parameters as before when using our training images.

```{r warning=FALSE}
path_test <- "test/" # setting path to our testing data

test_data_gen <- image_data_generator(rescale = 1/255)

test_images <- flow_images_from_directory(path_test,
                                          test_data_gen,
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = label_list,
                                          shuffle = F,
                                          seed = 220)

model %>% evaluate_generator(test_images, 
                             steps = test_images$n)
```

(I am going to ignore the error since we should have enough data; I'm not sure why it pops up)

So with our testing data, we also get around 80% accuracy (give or take a % or two). This could be improved, but shows that our model was not overfit to our training data.

For fun, I will use a candy wrapper that was not in our training data to test out how our model predicts it. Again, with how we setup the model beforehand, there is no need for me to "set up" the image beforehand - the model will do that for us.

![](testImage.jpg)

```{r}
test_image <- image_load("testImage.jpg",
                         target_size = target_size)

x <- image_to_array(test_image)
x <- array_reshape(x, c(1, dim(x)))
x <- x/255
pred <- model %>% predict(x)
pred <- data.frame("Type of Waste" = label_list, "Probability" = t(pred))
pred <- pred[order(pred$Probability, decreasing=T),][1:6,]
pred$Probability <- paste(format(100*pred$Probability,2),"%")
pred
```

Looks like our model predicted correctly, since this is a plastic wrapper.

Now I raise a question. Is there a particular type of material in our classification that our model does not accurately predict (which would be the main source of our lower accuracy)? The code block below will show us just that.

We will generate a matrix of all predictions for each image in our testing data for every material type. We label the highest prediction as the predicted class and then compare it with the actual correct class to get the % of correct classifications.

```{r warning=FALSE}
predictions <- model %>% 
  predict_generator(
    generator = test_images,
    steps = test_images$n 
  ) %>% as.data.frame

names(predictions) <- paste0("Class",0:5)

predictions$predicted_class <- 
  paste0("Class",apply(predictions,1,which.max)-1)
predictions$true_class <- paste0("Class",test_images$classes)

predictions %>% group_by(true_class) %>% 
  summarise(percentage_true = 100*sum(predicted_class == 
                                        true_class)/n()) %>% 
  left_join(data.frame(typeOfWaste = names(test_images$class_indices), 
                       true_class = paste0("Class",0:5)), by="true_class") %>%
  select(typeOfWaste, percentage_true) %>% 
  mutate(typeOfWaste = fct_reorder(typeOfWaste,percentage_true)) %>%
  ggplot(aes(x=typeOfWaste,y=percentage_true,fill=percentage_true, 
             label=percentage_true)) +
  geom_col() + theme_minimal() + coord_flip() +
  geom_text(nudge_y = 3) + 
  ggtitle("Percentage correct classifications by material")
```

We can see that the trash category does not appear to get accurate classifications. This makes sense as well, since this is a broad category with many images of different things that are not extremely similar with one another.

## Refining our model

So there are multiple ways to try and solve our problem with our model being poor accuracy. We could just try and gather more image data for the trash category, but given this is more of a broad category, this may not be that easy. So instead, we will try and tune the model we made. There are multiple ways, and for simplicity, I go with a very homemade approach, which is similar to `grid_search` in python. We will define a grid with values for some parameters that our model uses. We will just loop through every combination of parameters and fit the model, save the model combination, and final accuracy. We of course, want the parameters that result in the highest validation accuracy.

Note that on my computer, this whole process takes a while with our set of images. In the future, I may come back to this project and try more efficient methods to achieve this task, or test more sets of parameters, but since it is quite computationally intensive, I only test a very small grid of parameters.

```{r warning=FALSE}
tune_grid <- data.frame("dropoutrate" = c(0.3,0.2),
                        "n_dense" = c(1024,256))

tuning_results <- NULL

for (j in 1:length(tune_grid$dropoutrate)){
  for (k in 1:length(tune_grid$n_dense)){
      
      model <- model_function(
        # learning_rate = tune_grid$learning_rate[i], 
        dropoutrate = tune_grid$dropoutrate[j],
        n_dense = tune_grid$n_dense[k]
      )
      
      hist <- model %>% fit_generator(
        train_images,
        steps_per_epoch = train_images$n %/% batch_size, 
        epochs = epochs, 
        validation_data = validation_images,
        validation_steps = validation_images$n %/% 
          batch_size,
        verbose = 2
      )
      
      #Save model configurations
      tuning_results <- rbind(
        tuning_results,
        c("dropoutrate" = tune_grid$dropoutrate[j],
          "n_dense" = tune_grid$n_dense[k],
          "val_accuracy" = hist$metrics$val_accuracy))
      
  }
}
tuning_results
```

We can extract the best results:

```{r}
best_results <- tuning_results[which( 
  tuning_results[,ncol(tuning_results)] == 
    max(tuning_results[,ncol(tuning_results)])),]

best_results
```

## Finalizing our model

Now let us retrain our model with the best parameters we identified above. I also reduce the number of epochs to 5 to avoid potential over-fitting.

```{r}
model <- model_function(
                        dropoutrate = best_results["dropoutrate"],
                        n_dense = best_results["n_dense"])

hist <- model %>% fit_generator(
  train_images,
  steps_per_epoch = train_images$n %/% batch_size, 
  epochs = 5, 
  validation_data = validation_images,
  validation_steps = validation_images$n %/% batch_size,
  verbose = 2
)

```

Finally, we will save our model, and call this model in the second part of this project, where I incorporate this model into a dashboard.

```{r}
model %>% save_model_tf("recycleModel")
```
